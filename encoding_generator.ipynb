{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUFJlvmV/ZO6L/k9Um3rVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariquzzaman-faisal/hatespeech/blob/main/encoding_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting to drive"
      ],
      "metadata": {
        "id": "jlV0iz_glxBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HfGnhHtzRYKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98adb3c8-2924-4144-a36c-5cbc7d2c388b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "KJgphC_cQSqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install –upgrade setuptools\n",
        "!pip install git+https://github.com/csebuetnlp/normalizer\n",
        "!pip install pandas transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUgGcc-aQhGu",
        "outputId": "043bd999-f80a-4bb4-f556-bf248e76da18"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '–upgrade'\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/csebuetnlp/normalizer\n",
            "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-jk7cwqo7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-jk7cwqo7\n",
            "  Resolved https://github.com/csebuetnlp/normalizer to commit d80c3c484e1b80268f2b2dfaf7557fe65e34f321\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (2022.10.31)\n",
            "Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (1.4.2)\n",
            "Requirement already satisfied: ftfy==6.0.3 in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (6.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeNuNfbuN98B",
        "outputId": "f1bad47f-680f-44de-879b-66c4ec44c9a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "বাংলা টেক্সট স্যাম্পল এই টেক্সটে আমরা সাধারণ বাংলা বাক্যাংশ এবং পোস্টের জন্য এইচটিএমএল ট্যাগ, লিঙ্ক, হ্যাশট্যাগ এবং ইমোজি ব্যবহার করব। এই লিংকে ক্লিক করুন: সাম্পল লিঙ্ক হ্যাশট্যাগ: সাম্পল হ্যাশট্যাগ গুলির মধ্যে থাকছে: বাংলা টেক্সট এইচটিএমএল লিঙ্ক ইমোজি ইমোজি: আমরা এই বাংলা পোস্টের জন্য কিছু ইমোজি ব্যবহার করব।\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from normalizer import normalize\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_links(text):\n",
        "    clean_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    return clean_text\n",
        "\n",
        "# def remove_special_characters(text):\n",
        "#     clean_text = re.sub(r'[^\\w\\s০-৯\\s]', '', text)  # Include Bangla digits (০-৯)\n",
        "#     return clean_text\n",
        "\n",
        "def remove_hashtags(text):\n",
        "    clean_text = re.sub(r'#', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_emojis(text):\n",
        "    # Replace emojis and emoticons with an empty string\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        u\"\\U0001F000-\\U0001F0FF\"  # Mahjong Tiles\n",
        "        u\"\\U0001F200-\\U0001F2FF\"  # Enclosed Ideographic Supplement\n",
        "        u\"\\U0001F400-\\U0001F4FF\"  # Miscellaneous Symbols and Pictographs\n",
        "        u\"\\U0001F500-\\U0001F53F\"  # Emoticons (Emoji)\n",
        "        u\"\\U0001F540-\\U0001F543\"  # Transport and Map Symbols\n",
        "        u\"\\U0001F550-\\U0001F567\"  # Alchemical Symbols\n",
        "        u\"\\U0001F680-\\U0001F6C5\"  # Transport and Map Symbols Extended\n",
        "        u\"\\U0001F700-\\U0001F773\"  # Alchemical Symbols Extended\n",
        "        u\"\\U0001F780-\\U0001F7D4\"  # Geometric Shapes Extended-A\n",
        "        u\"\\U0001F800-\\U0001F80B\"  # Supplemental Arrows-C Extended\n",
        "        u\"\\U0001F810-\\U0001F847\"  # Supplemental Symbols and Pictographs Extended-B\n",
        "        u\"\\U0001F850-\\U0001F859\"  # Symbols and Pictographs Extended-C\n",
        "        u\"\\U0001F860-\\U0001F887\"  # Enclosed Ideographic Supplement Extended-A\n",
        "        u\"\\U0001F890-\\U0001F8AD\"  # Enclosed Ideographic Supplement Extended-B\n",
        "        u\"\\U0001F900-\\U0001F90B\"  # Supplemental Symbols and Pictographs Extended-C\n",
        "        u\"\\U0001F90D-\\U0001F971\"  # Emoji Additional Information\n",
        "        u\"\\U0001F973-\\U0001F97A\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F97C-\\U0001F9A2\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9A5-\\U0001F9AA\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9AE-\\U0001F9CA\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9CD-\\U0001F9FF\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001FA00-\\U0001FA6D\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001FA70-\\U0001FAD6\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F600-\\U0001F9FF\"  # Miscellaneous Symbols and Pictographs\n",
        "                           \"]\", flags=re.UNICODE)\n",
        "    clean_text = emoji_pattern.sub(r'', text)\n",
        "    return clean_text\n",
        "\n",
        "# def remove_duplicates(text):\n",
        "#     # Remove consecutive duplicate words\n",
        "#     words = text.split()\n",
        "#     unique_words = []\n",
        "#     for word in words:\n",
        "#         if word not in unique_words:\n",
        "#             unique_words.append(word)\n",
        "#     clean_text = ' '.join(unique_words)\n",
        "#     return clean_text\n",
        "\n",
        "# def remove_infrequent_words(text, min_df=5):\n",
        "#     # Remove tokens with a document frequency less than min_df\n",
        "#     words = text.split()\n",
        "#     word_freq = {}\n",
        "#     for word in words:\n",
        "#         word_freq[word] = word_freq.get(word, 0) + 1\n",
        "#     frequent_words = [word for word, freq in word_freq.items() if freq >= min_df]\n",
        "#     clean_text = ' '.join(frequent_words)\n",
        "#     return clean_text\n",
        "\n",
        "\n",
        "def remove_extra_whitespaces(input_string):\n",
        "    return re.sub(r'\\s+', ' ', input_string).strip()\n",
        "\n",
        "def preprocess_bangla_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_links(text)\n",
        "    # text = remove_special_characters(text)\n",
        "    text = remove_hashtags(text)\n",
        "    text = remove_emojis(text)\n",
        "    # text = remove_duplicates(text)\n",
        "    # text = remove_infrequent_words(text)\n",
        "    text = remove_extra_whitespaces(text)\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"\"\"\n",
        "    <h1>বাংলা টেক্সট স্যাম্পল</h1>\n",
        "    <p>এই টেক্সটে আমরা সাধারণ বাংলা বাক্যাংশ এবং পোস্টের জন্য এইচটিএমএল ট্যাগ, লিঙ্ক, হ্যাশট্যাগ এবং ইমোজি ব্যবহার করব।</p>\n",
        "\n",
        "    <p>এই লিংকে ক্লিক করুন: <a href=\"https://www.example.com\">সাম্পল লিঙ্ক</a></p>\n",
        "\n",
        "    <h2>হ্যাশট্যাগ:</h2>\n",
        "    <p>সাম্পল হ্যাশট্যাগ গুলির মধ্যে থাকছে: #বাংলা #টেক্সট #এইচটিএমএল #লিঙ্ক #ইমোজি</p>\n",
        "\n",
        "    <h2>ইমোজি:</h2>\n",
        "    <p>আমরা এই বাংলা পোস্টের জন্য কিছু ইমোজি ব্যবহার করব। 😊👍🎉💻📚</p>\n",
        "    \"\"\"\n",
        "preprocessed_text = preprocess_bangla_text(input_text)\n",
        "print(preprocessed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/bengali_hate_v2.0.csv\")\n",
        "# Extract text and label columns\n",
        "texts = df[\"text\"].tolist()\n",
        "labels = df[\"target\"].tolist()\n",
        "\n",
        "print(f'No of texts: {len(texts)}\\nNo of labels: {len(labels)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5z8GtAyqQ8D",
        "outputId": "24d46f99-9aeb-4463-8e4c-df175139ed28"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of texts: 5698\n",
            "No of labels: 5698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts = []\n",
        "for text in texts:\n",
        "    cleaned_text = preprocess_bangla_text(text)\n",
        "    cleaned_texts.append(cleaned_text)"
      ],
      "metadata": {
        "id": "_yyjfBTlqq8-"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaT8WCakrV4Z",
        "outputId": "d17bd5ee-540a-46f6-8616-c566ffe9e4ee"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['বৌদির দুধ দেকে তো আমার ই চোখ ঠিক ছিলো না - পোলাপান এর চোখ কিভাবে ঠিক থাকবে!',\n",
              " 'এই সরকার কে যারা নির্লজ্জের মত সাপোর্ট দিয়েছে বছরের পর বছর, তাদের আরো এমন রাস্তায় রাস্তায় কাঁদতে হবে',\n",
              " 'পিলখানা হত্যাকান্ড বাংলাদেশের প্রতিরক্ষা ব্যবস্থা ধ্বংসের জন্য ভারতের প্রত্যক্ষ সহযোগিতায় এই হত্যাকা- ঘটানো হয়েছিল',\n",
              " 'ভারতের অর্থনীতি নিয়ে আপনাদের ভাবতে হবে না। ভারতের অর্থনীতি নিয়ে ভারত সরকার আছে। আদার বেপারী জাহাজের খোঁজ নিয়ে লাভ নাই।',\n",
              " 'খানকির পুলা মালায়নদের মেরে সাফা করে ফেল']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating BanglaBERT Embeddings"
      ],
      "metadata": {
        "id": "HhMzmzuB7JPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForPreTraining, AutoTokenizer\n",
        "from normalizer import normalize\n",
        "import torch\n",
        "\n",
        "model = AutoModelForPreTraining.from_pretrained(\"csebuetnlp/banglabert\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")\n"
      ],
      "metadata": {
        "id": "nSgtR8itsgD8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_bert_embeddings(text):\n",
        "    text = normalize(text)\n",
        "    max_length = 30\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,      # Set the maximum sequence length\n",
        "        padding='max_length',       # Pad to the maximum length\n",
        "        truncation=True,            # Truncate if text exceeds max_length\n",
        "        return_tensors='pt'         # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Generate BERT embeddings\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = outputs.logits  # Get embeddings from the last hidden layer\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "bertEmbeddings = []\n",
        "for text in cleaned_texts:\n",
        "    embedding = get_bert_embeddings(text).numpy()\n",
        "    flattened_array = embedding.flatten()\n",
        "    bertEmbeddings.append(flattened_array)\n"
      ],
      "metadata": {
        "id": "kOISdeP_ycEe"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Embeddings: {len(bertEmbeddings)}')\n",
        "print(f'Targets: {len(labels)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWNAst2o6JZ7",
        "outputId": "64460aaf-8468-4c47-ff99-9c7038221905"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings: 5698\n",
            "Targets: 5698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bertEmbeddings)"
      ],
      "metadata": {
        "id": "zr-DLgA39IXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "data = zip(bertEmbeddings, labels)\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = \"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/embedded.csv\"\n",
        "\n",
        "# Write the data to the CSV file\n",
        "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"text\", \"label\"])  # Write the header\n",
        "    writer.writerows(data)  # Write the data rows"
      ],
      "metadata": {
        "id": "OR7J6yD27gq5"
      },
      "execution_count": 93,
      "outputs": []
    }
  ]
}