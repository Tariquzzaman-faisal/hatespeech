{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUFJlvmV/ZO6L/k9Um3rVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariquzzaman-faisal/hatespeech/blob/main/encoding_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting to drive"
      ],
      "metadata": {
        "id": "jlV0iz_glxBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HfGnhHtzRYKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98adb3c8-2924-4144-a36c-5cbc7d2c388b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "KJgphC_cQSqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ‚Äìupgrade setuptools\n",
        "!pip install git+https://github.com/csebuetnlp/normalizer\n",
        "!pip install pandas transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUgGcc-aQhGu",
        "outputId": "043bd999-f80a-4bb4-f556-bf248e76da18"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '‚Äìupgrade'\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/csebuetnlp/normalizer\n",
            "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-jk7cwqo7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-jk7cwqo7\n",
            "  Resolved https://github.com/csebuetnlp/normalizer to commit d80c3c484e1b80268f2b2dfaf7557fe65e34f321\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (2022.10.31)\n",
            "Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (1.4.2)\n",
            "Requirement already satisfied: ftfy==6.0.3 in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (6.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeNuNfbuN98B",
        "outputId": "f1bad47f-680f-44de-879b-66c4ec44c9a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∏‡ßç‡¶Ø‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶è‡¶á ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶è‡¶¨‡¶Ç ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó, ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï, ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶è‡¶¨‡¶Ç ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡•§ ‡¶è‡¶á ‡¶≤‡¶ø‡¶Ç‡¶ï‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®: ‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó: ‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶•‡¶æ‡¶ï‡¶õ‡ßá: ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø: ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶á ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡•§\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from normalizer import normalize\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_links(text):\n",
        "    clean_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    return clean_text\n",
        "\n",
        "# def remove_special_characters(text):\n",
        "#     clean_text = re.sub(r'[^\\w\\s‡ß¶-‡ßØ\\s]', '', text)  # Include Bangla digits (‡ß¶-‡ßØ)\n",
        "#     return clean_text\n",
        "\n",
        "def remove_hashtags(text):\n",
        "    clean_text = re.sub(r'#', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_emojis(text):\n",
        "    # Replace emojis and emoticons with an empty string\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        u\"\\U0001F000-\\U0001F0FF\"  # Mahjong Tiles\n",
        "        u\"\\U0001F200-\\U0001F2FF\"  # Enclosed Ideographic Supplement\n",
        "        u\"\\U0001F400-\\U0001F4FF\"  # Miscellaneous Symbols and Pictographs\n",
        "        u\"\\U0001F500-\\U0001F53F\"  # Emoticons (Emoji)\n",
        "        u\"\\U0001F540-\\U0001F543\"  # Transport and Map Symbols\n",
        "        u\"\\U0001F550-\\U0001F567\"  # Alchemical Symbols\n",
        "        u\"\\U0001F680-\\U0001F6C5\"  # Transport and Map Symbols Extended\n",
        "        u\"\\U0001F700-\\U0001F773\"  # Alchemical Symbols Extended\n",
        "        u\"\\U0001F780-\\U0001F7D4\"  # Geometric Shapes Extended-A\n",
        "        u\"\\U0001F800-\\U0001F80B\"  # Supplemental Arrows-C Extended\n",
        "        u\"\\U0001F810-\\U0001F847\"  # Supplemental Symbols and Pictographs Extended-B\n",
        "        u\"\\U0001F850-\\U0001F859\"  # Symbols and Pictographs Extended-C\n",
        "        u\"\\U0001F860-\\U0001F887\"  # Enclosed Ideographic Supplement Extended-A\n",
        "        u\"\\U0001F890-\\U0001F8AD\"  # Enclosed Ideographic Supplement Extended-B\n",
        "        u\"\\U0001F900-\\U0001F90B\"  # Supplemental Symbols and Pictographs Extended-C\n",
        "        u\"\\U0001F90D-\\U0001F971\"  # Emoji Additional Information\n",
        "        u\"\\U0001F973-\\U0001F97A\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F97C-\\U0001F9A2\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9A5-\\U0001F9AA\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9AE-\\U0001F9CA\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9CD-\\U0001F9FF\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001FA00-\\U0001FA6D\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001FA70-\\U0001FAD6\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F600-\\U0001F9FF\"  # Miscellaneous Symbols and Pictographs\n",
        "                           \"]\", flags=re.UNICODE)\n",
        "    clean_text = emoji_pattern.sub(r'', text)\n",
        "    return clean_text\n",
        "\n",
        "# def remove_duplicates(text):\n",
        "#     # Remove consecutive duplicate words\n",
        "#     words = text.split()\n",
        "#     unique_words = []\n",
        "#     for word in words:\n",
        "#         if word not in unique_words:\n",
        "#             unique_words.append(word)\n",
        "#     clean_text = ' '.join(unique_words)\n",
        "#     return clean_text\n",
        "\n",
        "# def remove_infrequent_words(text, min_df=5):\n",
        "#     # Remove tokens with a document frequency less than min_df\n",
        "#     words = text.split()\n",
        "#     word_freq = {}\n",
        "#     for word in words:\n",
        "#         word_freq[word] = word_freq.get(word, 0) + 1\n",
        "#     frequent_words = [word for word, freq in word_freq.items() if freq >= min_df]\n",
        "#     clean_text = ' '.join(frequent_words)\n",
        "#     return clean_text\n",
        "\n",
        "\n",
        "def remove_extra_whitespaces(input_string):\n",
        "    return re.sub(r'\\s+', ' ', input_string).strip()\n",
        "\n",
        "def preprocess_bangla_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_links(text)\n",
        "    # text = remove_special_characters(text)\n",
        "    text = remove_hashtags(text)\n",
        "    text = remove_emojis(text)\n",
        "    # text = remove_duplicates(text)\n",
        "    # text = remove_infrequent_words(text)\n",
        "    text = remove_extra_whitespaces(text)\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"\"\"\n",
        "    <h1>‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∏‡ßç‡¶Ø‡¶æ‡¶Æ‡ßç‡¶™‡¶≤</h1>\n",
        "    <p>‡¶è‡¶á ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶è‡¶¨‡¶Ç ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó, ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï, ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶è‡¶¨‡¶Ç ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡•§</p>\n",
        "\n",
        "    <p>‡¶è‡¶á ‡¶≤‡¶ø‡¶Ç‡¶ï‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®: <a href=\"https://www.example.com\">‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï</a></p>\n",
        "\n",
        "    <h2>‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó:</h2>\n",
        "    <p>‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶•‡¶æ‡¶ï‡¶õ‡ßá: #‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ #‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü #‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ #‡¶≤‡¶ø‡¶ô‡ßç‡¶ï #‡¶á‡¶Æ‡ßã‡¶ú‡¶ø</p>\n",
        "\n",
        "    <h2>‡¶á‡¶Æ‡ßã‡¶ú‡¶ø:</h2>\n",
        "    <p>‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶á ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡•§ üòäüëçüéâüíªüìö</p>\n",
        "    \"\"\"\n",
        "preprocessed_text = preprocess_bangla_text(input_text)\n",
        "print(preprocessed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/bengali_hate_v2.0.csv\")\n",
        "# Extract text and label columns\n",
        "texts = df[\"text\"].tolist()\n",
        "labels = df[\"target\"].tolist()\n",
        "\n",
        "print(f'No of texts: {len(texts)}\\nNo of labels: {len(labels)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5z8GtAyqQ8D",
        "outputId": "24d46f99-9aeb-4463-8e4c-df175139ed28"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of texts: 5698\n",
            "No of labels: 5698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts = []\n",
        "for text in texts:\n",
        "    cleaned_text = preprocess_bangla_text(text)\n",
        "    cleaned_texts.append(cleaned_text)"
      ],
      "metadata": {
        "id": "_yyjfBTlqq8-"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaT8WCakrV4Z",
        "outputId": "d17bd5ee-540a-46f6-8616-c566ffe9e4ee"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‡¶¨‡ßå‡¶¶‡¶ø‡¶∞ ‡¶¶‡ßÅ‡¶ß ‡¶¶‡ßá‡¶ï‡ßá ‡¶§‡ßã ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶á ‡¶ö‡ßã‡¶ñ ‡¶†‡¶ø‡¶ï ‡¶õ‡¶ø‡¶≤‡ßã ‡¶®‡¶æ - ‡¶™‡ßã‡¶≤‡¶æ‡¶™‡¶æ‡¶® ‡¶è‡¶∞ ‡¶ö‡ßã‡¶ñ ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶†‡¶ø‡¶ï ‡¶•‡¶æ‡¶ï‡¶¨‡ßá!',\n",
              " '‡¶è‡¶á ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶ï‡ßá ‡¶Ø‡¶æ‡¶∞‡¶æ ‡¶®‡¶ø‡¶∞‡ßç‡¶≤‡¶ú‡ßç‡¶ú‡ßá‡¶∞ ‡¶Æ‡¶§ ‡¶∏‡¶æ‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßá ‡¶¨‡¶õ‡¶∞‡ßá‡¶∞ ‡¶™‡¶∞ ‡¶¨‡¶õ‡¶∞, ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ü‡¶∞‡ßã ‡¶è‡¶Æ‡¶® ‡¶∞‡¶æ‡¶∏‡ßç‡¶§‡¶æ‡ßü ‡¶∞‡¶æ‡¶∏‡ßç‡¶§‡¶æ‡ßü ‡¶ï‡¶æ‡¶Å‡¶¶‡¶§‡ßá ‡¶π‡¶¨‡ßá',\n",
              " '‡¶™‡¶ø‡¶≤‡¶ñ‡¶æ‡¶®‡¶æ ‡¶π‡¶§‡ßç‡¶Ø‡¶æ‡¶ï‡¶æ‡¶®‡ßç‡¶° ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∞‡¶ï‡ßç‡¶∑‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ ‡¶ß‡ßç‡¶¨‡¶Ç‡¶∏‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡¶ï‡ßç‡¶∑ ‡¶∏‡¶π‡¶Ø‡ßã‡¶ó‡¶ø‡¶§‡¶æ‡ßü ‡¶è‡¶á ‡¶π‡¶§‡ßç‡¶Ø‡¶æ‡¶ï‡¶æ- ‡¶ò‡¶ü‡¶æ‡¶®‡ßã ‡¶π‡ßü‡ßá‡¶õ‡¶ø‡¶≤',\n",
              " '‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≠‡¶æ‡¶¨‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡¶®‡¶æ‡•§ ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶≠‡¶æ‡¶∞‡¶§ ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶Ü‡¶õ‡ßá‡•§ ‡¶Ü‡¶¶‡¶æ‡¶∞ ‡¶¨‡ßá‡¶™‡¶æ‡¶∞‡ßÄ ‡¶ú‡¶æ‡¶π‡¶æ‡¶ú‡ßá‡¶∞ ‡¶ñ‡ßã‡¶Å‡¶ú ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶≤‡¶æ‡¶≠ ‡¶®‡¶æ‡¶á‡•§',\n",
              " '‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞ ‡¶™‡ßÅ‡¶≤‡¶æ ‡¶Æ‡¶æ‡¶≤‡¶æ‡ßü‡¶®‡¶¶‡ßá‡¶∞ ‡¶Æ‡ßá‡¶∞‡ßá ‡¶∏‡¶æ‡¶´‡¶æ ‡¶ï‡¶∞‡ßá ‡¶´‡ßá‡¶≤']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating BanglaBERT Embeddings"
      ],
      "metadata": {
        "id": "HhMzmzuB7JPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForPreTraining, AutoTokenizer\n",
        "from normalizer import normalize\n",
        "import torch\n",
        "\n",
        "model = AutoModelForPreTraining.from_pretrained(\"csebuetnlp/banglabert\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")\n"
      ],
      "metadata": {
        "id": "nSgtR8itsgD8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_bert_embeddings(text):\n",
        "    text = normalize(text)\n",
        "    max_length = 30\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,      # Set the maximum sequence length\n",
        "        padding='max_length',       # Pad to the maximum length\n",
        "        truncation=True,            # Truncate if text exceeds max_length\n",
        "        return_tensors='pt'         # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Generate BERT embeddings\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = outputs.logits  # Get embeddings from the last hidden layer\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "bertEmbeddings = []\n",
        "for text in cleaned_texts:\n",
        "    embedding = get_bert_embeddings(text).numpy()\n",
        "    flattened_array = embedding.flatten()\n",
        "    bertEmbeddings.append(flattened_array)\n"
      ],
      "metadata": {
        "id": "kOISdeP_ycEe"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Embeddings: {len(bertEmbeddings)}')\n",
        "print(f'Targets: {len(labels)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWNAst2o6JZ7",
        "outputId": "64460aaf-8468-4c47-ff99-9c7038221905"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings: 5698\n",
            "Targets: 5698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bertEmbeddings)"
      ],
      "metadata": {
        "id": "zr-DLgA39IXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "data = zip(bertEmbeddings, labels)\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = \"/content/drive/MyDrive/Research/Shared Task/Violence Inciting Text Detection (VITD) Bangla/embedded.csv\"\n",
        "\n",
        "# Write the data to the CSV file\n",
        "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"text\", \"label\"])  # Write the header\n",
        "    writer.writerows(data)  # Write the data rows"
      ],
      "metadata": {
        "id": "OR7J6yD27gq5"
      },
      "execution_count": 93,
      "outputs": []
    }
  ]
}