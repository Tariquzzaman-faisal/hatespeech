{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw+SBpu7rgVSPAXP0R3Ej0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariquzzaman-faisal/hatespeech/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "KJgphC_cQSqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ‚Äìupgrade setuptools\n",
        "!pip install bnltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUgGcc-aQhGu",
        "outputId": "527a5095-9331-4ccb-8136-2278d7bac0a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '‚Äìupgrade'\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting bnltk\n",
            "  Using cached bnltk-0.7.6-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from bnltk) (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from bnltk) (2.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bnltk) (1.22.4)\n",
            "Collecting sklearn (from bnltk)\n",
            "  Using cached sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bnltk) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bnltk) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bnltk) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bnltk) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bnltk) (3.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (0.4.13)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->bnltk) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->bnltk) (0.41.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->bnltk) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->bnltk) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->bnltk) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->bnltk) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->bnltk) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->bnltk) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->bnltk) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->bnltk) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->bnltk) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->bnltk) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->bnltk) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->bnltk) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->bnltk) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->bnltk) (3.2.2)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2950 sha256=c004c603171ae40bf1b30871f21159463649489c9f21fb2066869be9f8d5f6ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn, bnltk\n",
            "Successfully installed bnltk-0.7.6 sklearn-0.0.post7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeNuNfbuN98B",
        "outputId": "853c8768-c7dc-4933-9ab6-95b60c1d0efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∏‡ßç‡¶Ø‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶è ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶Ü‡¶Æ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶è‡¶¨‡¶Ç ‡¶™‡ßã‡¶∏‡ßç‡¶ü ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó, ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï, ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶è‡¶¨‡¶Ç ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π ‡¶ï‡¶∞‡¶¨‡•§ ‡¶è ‡¶≤‡¶ø‡¶Ç ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®: ‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó: ‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø ‡¶•‡¶æ‡¶ï‡¶õ‡ßá: ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø: ‡¶Ü‡¶Æ ‡¶è ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶™‡ßã‡¶∏‡ßç‡¶ü ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π ‡¶ï‡¶∞‡¶¨‡•§\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from bnltk.stemmer import BanglaStemmer\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_links(text):\n",
        "    clean_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    return clean_text\n",
        "\n",
        "# def remove_special_characters(text):\n",
        "#     clean_text = re.sub(r'[^\\w\\s‡ß¶-‡ßØ\\s]', '', text)  # Include Bangla digits (‡ß¶-‡ßØ)\n",
        "#     return clean_text\n",
        "\n",
        "def remove_hashtags(text):\n",
        "    clean_text = re.sub(r'#', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_emojis(text):\n",
        "    # Replace emojis and emoticons with an empty string\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        u\"\\U0001F000-\\U0001F0FF\"  # Mahjong Tiles\n",
        "        u\"\\U0001F200-\\U0001F2FF\"  # Enclosed Ideographic Supplement\n",
        "        u\"\\U0001F400-\\U0001F4FF\"  # Miscellaneous Symbols and Pictographs\n",
        "        u\"\\U0001F500-\\U0001F53F\"  # Emoticons (Emoji)\n",
        "        u\"\\U0001F540-\\U0001F543\"  # Transport and Map Symbols\n",
        "        u\"\\U0001F550-\\U0001F567\"  # Alchemical Symbols\n",
        "        u\"\\U0001F680-\\U0001F6C5\"  # Transport and Map Symbols Extended\n",
        "        u\"\\U0001F700-\\U0001F773\"  # Alchemical Symbols Extended\n",
        "        u\"\\U0001F780-\\U0001F7D4\"  # Geometric Shapes Extended-A\n",
        "        u\"\\U0001F800-\\U0001F80B\"  # Supplemental Arrows-C Extended\n",
        "        u\"\\U0001F810-\\U0001F847\"  # Supplemental Symbols and Pictographs Extended-B\n",
        "        u\"\\U0001F850-\\U0001F859\"  # Symbols and Pictographs Extended-C\n",
        "        u\"\\U0001F860-\\U0001F887\"  # Enclosed Ideographic Supplement Extended-A\n",
        "        u\"\\U0001F890-\\U0001F8AD\"  # Enclosed Ideographic Supplement Extended-B\n",
        "        u\"\\U0001F900-\\U0001F90B\"  # Supplemental Symbols and Pictographs Extended-C\n",
        "        u\"\\U0001F90D-\\U0001F971\"  # Emoji Additional Information\n",
        "        u\"\\U0001F973-\\U0001F97A\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F97C-\\U0001F9A2\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9A5-\\U0001F9AA\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9AE-\\U0001F9CA\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F9CD-\\U0001F9FF\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001FA00-\\U0001FA6D\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001FA70-\\U0001FAD6\"  # Emoji Additional Information-Supplement\n",
        "        u\"\\U0001F600-\\U0001F9FF\"  # Miscellaneous Symbols and Pictographs\n",
        "                           \"]\", flags=re.UNICODE)\n",
        "    clean_text = emoji_pattern.sub(r'', text)\n",
        "    return clean_text\n",
        "\n",
        "# def remove_duplicates(text):\n",
        "#     # Remove consecutive duplicate words\n",
        "#     words = text.split()\n",
        "#     unique_words = []\n",
        "#     for word in words:\n",
        "#         if word not in unique_words:\n",
        "#             unique_words.append(word)\n",
        "#     clean_text = ' '.join(unique_words)\n",
        "#     return clean_text\n",
        "\n",
        "# def remove_infrequent_words(text, min_df=5):\n",
        "#     # Remove tokens with a document frequency less than min_df\n",
        "#     words = text.split()\n",
        "#     word_freq = {}\n",
        "#     for word in words:\n",
        "#         word_freq[word] = word_freq.get(word, 0) + 1\n",
        "#     frequent_words = [word for word, freq in word_freq.items() if freq >= min_df]\n",
        "#     clean_text = ' '.join(frequent_words)\n",
        "#     return clean_text\n",
        "\n",
        "def stem_bangla_words(text):\n",
        "    bn_stemmer = BanglaStemmer()\n",
        "    words = text.split()\n",
        "    stemmed_words = [bn_stemmer.stem(word) for word in words]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "def preprocess_bangla_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_links(text)\n",
        "    # text = remove_special_characters(text)\n",
        "    text = remove_hashtags(text)\n",
        "    text = remove_emojis(text)\n",
        "    # text = remove_duplicates(text)\n",
        "    # text = remove_infrequent_words(text)\n",
        "    text = stem_bangla_words(text)\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"\"\"\n",
        "    <h1>‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∏‡ßç‡¶Ø‡¶æ‡¶Æ‡ßç‡¶™‡¶≤</h1>\n",
        "    <p>‡¶è‡¶á ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶è‡¶¨‡¶Ç ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó, ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï, ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶è‡¶¨‡¶Ç ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡•§</p>\n",
        "\n",
        "    <p>‡¶è‡¶á ‡¶≤‡¶ø‡¶Ç‡¶ï‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®: <a href=\"https://www.example.com\">‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï</a></p>\n",
        "\n",
        "    <h2>‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó:</h2>\n",
        "    <p>‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡¶≤ ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶•‡¶æ‡¶ï‡¶õ‡ßá: #‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ #‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü #‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ #‡¶≤‡¶ø‡¶ô‡ßç‡¶ï #‡¶á‡¶Æ‡ßã‡¶ú‡¶ø</p>\n",
        "\n",
        "    <h2>‡¶á‡¶Æ‡ßã‡¶ú‡¶ø:</h2>\n",
        "    <p>‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶á ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡•§ üòäüëçüéâüíªüìö</p>\n",
        "    \"\"\"\n",
        "preprocessed_text = preprocess_bangla_text(input_text)\n",
        "print(preprocessed_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bnltk.tokenize import Tokenizers\n",
        "t = Tokenizers()\n",
        "print(t.bn_word_tokenizer('‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤ ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó, ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï, ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶è‡¶¨‡¶Ç ‡¶á‡¶Æ‡ßã‡¶ú‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π ‡¶ï‡¶∞‡¶¨‡•§ ‡¶è ‡¶≤‡¶ø‡¶Ç ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®: '))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nTOoab0RXv9",
        "outputId": "efd62ea1-8b4a-4632-e286-5bab07efede6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‡¶è‡¶á‡¶ö‡¶ü‡¶ø‡¶è‡¶Æ‡¶è‡¶≤', '‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó', '‡¶≤‡¶ø‡¶ô‡ßç‡¶ï', '‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó', '‡¶è‡¶¨‡¶Ç', '‡¶á‡¶Æ‡ßã‡¶ú‡¶ø', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π', '‡¶ï‡¶∞‡¶¨', '‡¶è', '‡¶≤‡¶ø‡¶Ç', '‡¶ï‡ßç‡¶≤‡¶ø‡¶ï', '‡¶ï‡¶∞‡ßÅ‡¶®']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfGnhHtzRYKb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}